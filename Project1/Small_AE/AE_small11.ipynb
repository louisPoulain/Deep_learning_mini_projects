{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n",
      "Training data : \n",
      " noisy_imgs_1 :  torch.Size([50000, 3, 32, 32]) \n",
      " noisy_imgs_2 :  torch.Size([50000, 3, 32, 32])\n",
      "Data reduced : \n",
      " noisy_imgs_1_reduced :  torch.Size([50000, 3, 32, 32]) \n",
      " noisy_imgs_2_reduced :  torch.Size([50000, 3, 32, 32])\n",
      "Type :  torch.uint8\n",
      "epoch :  1\n",
      "The epoch took 81.6002242565155s to complete\n",
      "\n",
      "epoch :  2\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\louis\\Documents\\EPFL\\Master2\\Deep_learning_mini_projects\\Project1\\Small_AE\\AE_small11.ipynb Cell 1'\u001b[0m in \u001b[0;36m<cell line: 292>\u001b[1;34m()\u001b[0m\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/louis/Documents/EPFL/Master2/Deep_learning_mini_projects/Project1/Small_AE/AE_small11.ipynb#ch0000000?line=294'>295</a>\u001b[0m Loss \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/louis/Documents/EPFL/Master2/Deep_learning_mini_projects/Project1/Small_AE/AE_small11.ipynb#ch0000000?line=295'>296</a>\u001b[0m \u001b[39mfor\u001b[39;00m noisy_imgs_1, noisy_imgs_2 \u001b[39min\u001b[39;00m loader_1:\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/louis/Documents/EPFL/Master2/Deep_learning_mini_projects/Project1/Small_AE/AE_small11.ipynb#ch0000000?line=296'>297</a>\u001b[0m     \u001b[39m#print(noisy_imgs_1.shape)\u001b[39;00m\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/louis/Documents/EPFL/Master2/Deep_learning_mini_projects/Project1/Small_AE/AE_small11.ipynb#ch0000000?line=297'>298</a>\u001b[0m     \u001b[39m#print(noisy_imgs_2.shape)\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/louis/Documents/EPFL/Master2/Deep_learning_mini_projects/Project1/Small_AE/AE_small11.ipynb#ch0000000?line=301'>302</a>\u001b[0m     \u001b[39m# Output of Autoencoder\u001b[39;00m\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/louis/Documents/EPFL/Master2/Deep_learning_mini_projects/Project1/Small_AE/AE_small11.ipynb#ch0000000?line=302'>303</a>\u001b[0m     \u001b[39m#print(\"type : \", noisy_imgs_1.dtype)\u001b[39;00m\n\u001b[1;32m--> <a href='vscode-notebook-cell:/c%3A/Users/louis/Documents/EPFL/Master2/Deep_learning_mini_projects/Project1/Small_AE/AE_small11.ipynb#ch0000000?line=303'>304</a>\u001b[0m     reconstructed \u001b[39m=\u001b[39m model(noisy_imgs_1)\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/louis/Documents/EPFL/Master2/Deep_learning_mini_projects/Project1/Small_AE/AE_small11.ipynb#ch0000000?line=305'>306</a>\u001b[0m     \u001b[39m# Calculating the loss function\u001b[39;00m\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/louis/Documents/EPFL/Master2/Deep_learning_mini_projects/Project1/Small_AE/AE_small11.ipynb#ch0000000?line=306'>307</a>\u001b[0m     loss \u001b[39m=\u001b[39m loss_function(reconstructed, noisy_imgs_2)\n",
      "File \u001b[1;32mc:\\Users\\louis\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   <a href='file:///c%3A/Users/louis/AppData/Local/Programs/Python/Python310/lib/site-packages/torch/nn/modules/module.py?line=1105'>1106</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/louis/AppData/Local/Programs/Python/Python310/lib/site-packages/torch/nn/modules/module.py?line=1106'>1107</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/louis/AppData/Local/Programs/Python/Python310/lib/site-packages/torch/nn/modules/module.py?line=1107'>1108</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   <a href='file:///c%3A/Users/louis/AppData/Local/Programs/Python/Python310/lib/site-packages/torch/nn/modules/module.py?line=1108'>1109</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> <a href='file:///c%3A/Users/louis/AppData/Local/Programs/Python/Python310/lib/site-packages/torch/nn/modules/module.py?line=1109'>1110</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   <a href='file:///c%3A/Users/louis/AppData/Local/Programs/Python/Python310/lib/site-packages/torch/nn/modules/module.py?line=1110'>1111</a>\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/louis/AppData/Local/Programs/Python/Python310/lib/site-packages/torch/nn/modules/module.py?line=1111'>1112</a>\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\u001b[1;32mc:\\Users\\louis\\Documents\\EPFL\\Master2\\Deep_learning_mini_projects\\Project1\\Small_AE\\AE_small11.ipynb Cell 1'\u001b[0m in \u001b[0;36mAE.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/louis/Documents/EPFL/Master2/Deep_learning_mini_projects/Project1/Small_AE/AE_small11.ipynb#ch0000000?line=173'>174</a>\u001b[0m y2_1 \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mcat((x2_5, x2_4), dim \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m)\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/louis/Documents/EPFL/Master2/Deep_learning_mini_projects/Project1/Small_AE/AE_small11.ipynb#ch0000000?line=174'>175</a>\u001b[0m y2_1 \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39ml_relu(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mupsample(y2_1))\n\u001b[1;32m--> <a href='vscode-notebook-cell:/c%3A/Users/louis/Documents/EPFL/Master2/Deep_learning_mini_projects/Project1/Small_AE/AE_small11.ipynb#ch0000000?line=175'>176</a>\u001b[0m y2_1 \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39ml_relu(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdeconv2_1(y2_1))\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/louis/Documents/EPFL/Master2/Deep_learning_mini_projects/Project1/Small_AE/AE_small11.ipynb#ch0000000?line=176'>177</a>\u001b[0m \u001b[39m#print(y1.shape)\u001b[39;00m\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/louis/Documents/EPFL/Master2/Deep_learning_mini_projects/Project1/Small_AE/AE_small11.ipynb#ch0000000?line=177'>178</a>\u001b[0m y2_2 \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mcat((y2_1, x2_3), dim \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\louis\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   <a href='file:///c%3A/Users/louis/AppData/Local/Programs/Python/Python310/lib/site-packages/torch/nn/modules/module.py?line=1105'>1106</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/louis/AppData/Local/Programs/Python/Python310/lib/site-packages/torch/nn/modules/module.py?line=1106'>1107</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/louis/AppData/Local/Programs/Python/Python310/lib/site-packages/torch/nn/modules/module.py?line=1107'>1108</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   <a href='file:///c%3A/Users/louis/AppData/Local/Programs/Python/Python310/lib/site-packages/torch/nn/modules/module.py?line=1108'>1109</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> <a href='file:///c%3A/Users/louis/AppData/Local/Programs/Python/Python310/lib/site-packages/torch/nn/modules/module.py?line=1109'>1110</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   <a href='file:///c%3A/Users/louis/AppData/Local/Programs/Python/Python310/lib/site-packages/torch/nn/modules/module.py?line=1110'>1111</a>\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/louis/AppData/Local/Programs/Python/Python310/lib/site-packages/torch/nn/modules/module.py?line=1111'>1112</a>\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\louis\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\conv.py:447\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/louis/AppData/Local/Programs/Python/Python310/lib/site-packages/torch/nn/modules/conv.py?line=445'>446</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[1;32m--> <a href='file:///c%3A/Users/louis/AppData/Local/Programs/Python/Python310/lib/site-packages/torch/nn/modules/conv.py?line=446'>447</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_conv_forward(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "File \u001b[1;32mc:\\Users\\louis\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\conv.py:440\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[1;34m(self, input, weight, bias)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/louis/AppData/Local/Programs/Python/Python310/lib/site-packages/torch/nn/modules/conv.py?line=437'>438</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_conv_forward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor, weight: Tensor, bias: Optional[Tensor]):\n\u001b[0;32m    <a href='file:///c%3A/Users/louis/AppData/Local/Programs/Python/Python310/lib/site-packages/torch/nn/modules/conv.py?line=438'>439</a>\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode \u001b[39m!=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mzeros\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[1;32m--> <a href='file:///c%3A/Users/louis/AppData/Local/Programs/Python/Python310/lib/site-packages/torch/nn/modules/conv.py?line=439'>440</a>\u001b[0m         \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mconv2d(F\u001b[39m.\u001b[39;49mpad(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_reversed_padding_repeated_twice, mode\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpadding_mode),\n\u001b[0;32m    <a href='file:///c%3A/Users/louis/AppData/Local/Programs/Python/Python310/lib/site-packages/torch/nn/modules/conv.py?line=440'>441</a>\u001b[0m                         weight, bias, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstride,\n\u001b[0;32m    <a href='file:///c%3A/Users/louis/AppData/Local/Programs/Python/Python310/lib/site-packages/torch/nn/modules/conv.py?line=441'>442</a>\u001b[0m                         _pair(\u001b[39m0\u001b[39;49m), \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdilation, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgroups)\n\u001b[0;32m    <a href='file:///c%3A/Users/louis/AppData/Local/Programs/Python/Python310/lib/site-packages/torch/nn/modules/conv.py?line=442'>443</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39mconv2d(\u001b[39minput\u001b[39m, weight, bias, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstride,\n\u001b[0;32m    <a href='file:///c%3A/Users/louis/AppData/Local/Programs/Python/Python310/lib/site-packages/torch/nn/modules/conv.py?line=443'>444</a>\u001b[0m                     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdilation, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgroups)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Wed Apr 27 15:43:57 2022\n",
    "\n",
    "@author: louis\n",
    "\"\"\"\n",
    "\"file for colab\"\n",
    "import torch \n",
    "# import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def psnr(denoised, ground_truth):\n",
    "    # Peak Signal to Noise Ratio: denoised and ground Ì‡truth have range [0, 1]\n",
    "    mse = torch.mean((denoised - ground_truth) ** 2)\n",
    "    return -10 * torch.log10(mse + 10**-8)\n",
    "\n",
    "class Dataset(torch.utils.data.Dataset):\n",
    "  'Characterizes a dataset for PyTorch'\n",
    "  def __init__(self, SIZE, train = True):\n",
    "        'Initialization'\n",
    "        if train: \n",
    "            x, y = torch.load(\"./train_data.pkl\") #drive/MyDrive/Colab_Notebooks/DeepL_miniProj\n",
    "            print(\"Training data : \\n noisy_imgs_1 : \", x.shape, \"\\n noisy_imgs_2 : \", y.shape)\n",
    "            if SIZE > 50000:\n",
    "                print(\"You entered a size too big, using size = 50000\")\n",
    "                SIZE = 50000\n",
    "        else : \n",
    "            x, y = torch.load(\"./val_data.pkl\")\n",
    "            print(\"Test data : \\n noisy_imgs : \", x.shape, \"\\n clean_imgs : \", y.shape)\n",
    "            if SIZE > 50000:\n",
    "                print(\"You entered a size too big, using size = 1000\")\n",
    "                SIZE = 1000\n",
    "        x, y = x[:SIZE], y[:SIZE]\n",
    "        print(\"Data reduced : \\n noisy_imgs_1_reduced : \", x.shape, \"\\n noisy_imgs_2_reduced : \", y.shape)\n",
    "        print(\"Type : \", x.dtype)\n",
    "        self.x = x.float()\n",
    "        self.y = y.float()\n",
    "\n",
    "  def __len__(self):\n",
    "        'Denotes the total number of samples'\n",
    "        return len(self.x)\n",
    "\n",
    "  def __getitem__(self, index):\n",
    "        'Generates one sample of data'\n",
    "        # get label\n",
    "        X = self.x[index]\n",
    "        Y = self.y[index]\n",
    "        return X, Y\n",
    "    \n",
    "class AE(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        # channel per channel operation\n",
    "        pad_mode = 'replicate'\n",
    "        super().__init__()\n",
    "        self.pool = nn.MaxPool2d(kernel_size = 2)\n",
    "        self.l_relu = nn.LeakyReLU(negative_slope = 0.1)\n",
    "        self.upsample = nn.Upsample(scale_factor = (2, 2))\n",
    "        self.conv1_1 = nn.Conv2d(in_channels = 1, out_channels = 16, kernel_size = 3, padding = 1, padding_mode = pad_mode) # 32 x 32 \n",
    "        self.conv1_2 = nn.Conv2d(in_channels = 16, out_channels = 24, kernel_size = 3, padding = 1, padding_mode = pad_mode) \n",
    "        self.conv1_3 = nn.Conv2d(in_channels = 24, out_channels = 48, kernel_size = 3, padding = 1, padding_mode = pad_mode) \n",
    "        self.conv1_4 = nn.Conv2d(in_channels = 48, out_channels = 48, kernel_size = 3, padding = 1, padding_mode = pad_mode) \n",
    "        self.conv1_5 = nn.Conv2d(in_channels = 48, out_channels = 48, kernel_size = 3, padding = 1, padding_mode = pad_mode) \n",
    "        \n",
    "        self.deconv1_1 = nn.Conv2d(in_channels = 96, out_channels = 48, kernel_size = 3, padding = 1, padding_mode = pad_mode) \n",
    "        self.deconv1_2 = nn.Conv2d(in_channels = 96, out_channels = 48, kernel_size = 3, padding = 1, padding_mode = pad_mode) \n",
    "        self.deconv1_3 = nn.Conv2d(in_channels = 72, out_channels = 24, kernel_size = 3, padding = 1, padding_mode = pad_mode) \n",
    "        self.deconv1_4 = nn.Conv2d(in_channels = 40, out_channels = 16, kernel_size = 3, padding = 1, padding_mode = pad_mode) \n",
    "        self.deconv1_5 = nn.Conv2d(in_channels = 17, out_channels = 1, kernel_size = 3, padding = 1, padding_mode = pad_mode) \n",
    "        \n",
    "        self.conv2_1 = nn.Conv2d(in_channels = 1, out_channels = 16, kernel_size = 3, padding = 1, padding_mode = pad_mode) # 32 x 32 \n",
    "        self.conv2_2 = nn.Conv2d(in_channels = 16, out_channels = 24, kernel_size = 3, padding = 1, padding_mode = pad_mode) \n",
    "        self.conv2_3 = nn.Conv2d(in_channels = 24, out_channels = 48, kernel_size = 3, padding = 1, padding_mode = pad_mode) \n",
    "        self.conv2_4 = nn.Conv2d(in_channels = 48, out_channels = 48, kernel_size = 3, padding = 1, padding_mode = pad_mode) \n",
    "        self.conv2_5 = nn.Conv2d(in_channels = 48, out_channels = 48, kernel_size = 3, padding = 1, padding_mode = pad_mode) \n",
    "        \n",
    "        self.deconv2_1 = nn.Conv2d(in_channels = 96, out_channels = 48, kernel_size = 3, padding = 1, padding_mode = pad_mode) \n",
    "        self.deconv2_2 = nn.Conv2d(in_channels = 96, out_channels = 48, kernel_size = 3, padding = 1, padding_mode = pad_mode) \n",
    "        self.deconv2_3 = nn.Conv2d(in_channels = 72, out_channels = 24, kernel_size = 3, padding = 1, padding_mode = pad_mode) \n",
    "        self.deconv2_4 = nn.Conv2d(in_channels = 40, out_channels = 16, kernel_size = 3, padding = 1, padding_mode = pad_mode) \n",
    "        self.deconv2_5 = nn.Conv2d(in_channels = 17, out_channels = 1, kernel_size = 3, padding = 1, padding_mode = pad_mode)\n",
    "\n",
    "        self.conv3_1 = nn.Conv2d(in_channels = 1, out_channels = 16, kernel_size = 3, padding = 1, padding_mode = pad_mode) # 32 x 32 \n",
    "        self.conv3_2 = nn.Conv2d(in_channels = 16, out_channels = 24, kernel_size = 3, padding = 1, padding_mode = pad_mode) \n",
    "        self.conv3_3 = nn.Conv2d(in_channels = 24, out_channels = 48, kernel_size = 3, padding = 1, padding_mode = pad_mode) \n",
    "        self.conv3_4 = nn.Conv2d(in_channels = 48, out_channels = 48, kernel_size = 3, padding = 1, padding_mode = pad_mode) \n",
    "        self.conv3_5 = nn.Conv2d(in_channels = 48, out_channels = 48, kernel_size = 3, padding = 1, padding_mode = pad_mode) \n",
    "        \n",
    "        self.deconv3_1 = nn.Conv2d(in_channels = 96, out_channels = 48, kernel_size = 3, padding = 1, padding_mode = pad_mode) \n",
    "        self.deconv3_2 = nn.Conv2d(in_channels = 96, out_channels = 48, kernel_size = 3, padding = 1, padding_mode = pad_mode) \n",
    "        self.deconv3_3 = nn.Conv2d(in_channels = 72, out_channels = 24, kernel_size = 3, padding = 1, padding_mode = pad_mode) \n",
    "        self.deconv3_4 = nn.Conv2d(in_channels = 40, out_channels = 16, kernel_size = 3, padding = 1, padding_mode = pad_mode) \n",
    "        self.deconv3_5 = nn.Conv2d(in_channels = 17, out_channels = 1, kernel_size = 3, padding = 1, padding_mode = pad_mode)\n",
    "        \n",
    "  \n",
    "    def forward(self, x):\n",
    "        # Extract the 3 channels\n",
    "        x1 = x[:, 0, :, :].reshape((x.shape[0], 1, x.shape[2], -1))\n",
    "        x2 = x[:, 1, :, :].reshape((x.shape[0], 1, x.shape[2], -1))\n",
    "        x3 = x[:, 2, :, :].reshape((x.shape[0], 1, x.shape[2], -1))\n",
    "        #print(x1.shape, x2.shape, x3.shape)\n",
    "\n",
    "        # Encode separately each channel\n",
    "        # First channel\n",
    "        x1_1 = self.l_relu(self.conv1_1(x1))\n",
    "        x1_1 = self.l_relu(self.pool(x1_1))\n",
    "\n",
    "        x1_2 = self.l_relu(self.conv1_2(x1_1))\n",
    "        x1_2 = self.l_relu(self.pool(x1_2))\n",
    "\n",
    "        x1_3 = self.l_relu(self.conv1_3(x1_2))\n",
    "        x1_3 = self.l_relu(self.pool(x1_3))\n",
    "\n",
    "        x1_4 = self.l_relu(self.conv1_4(x1_3))\n",
    "        x1_4 = self.l_relu(self.pool(x1_4))\n",
    "\n",
    "        x1_5 = self.l_relu(self.conv1_5(x1_4))\n",
    "        #print(x5.shape)\n",
    "\n",
    "        # Second channel\n",
    "        x2_1 = self.l_relu(self.conv2_1(x2))\n",
    "        x2_1 = self.l_relu(self.pool(x2_1))\n",
    "\n",
    "        x2_2 = self.l_relu(self.conv2_2(x2_1))\n",
    "        x2_2 = self.l_relu(self.pool(x2_2))\n",
    "\n",
    "        x2_3 = self.l_relu(self.conv2_3(x2_2))\n",
    "        x2_3 = self.l_relu(self.pool(x2_3))\n",
    "\n",
    "        x2_4 = self.l_relu(self.conv2_4(x2_3))\n",
    "        x2_4 = self.l_relu(self.pool(x2_4))\n",
    "\n",
    "        x2_5 = self.l_relu(self.conv2_5(x2_4))\n",
    "\n",
    "        # Third channel\n",
    "        x3_1 = self.l_relu(self.conv3_1(x3))\n",
    "        x3_1 = self.l_relu(self.pool(x3_1))\n",
    "\n",
    "        x3_2 = self.l_relu(self.conv3_2(x3_1))\n",
    "        x3_2 = self.l_relu(self.pool(x3_2))\n",
    "\n",
    "        x3_3 = self.l_relu(self.conv3_3(x3_2))\n",
    "        x3_3 = self.l_relu(self.pool(x3_3))\n",
    "\n",
    "        x3_4 = self.l_relu(self.conv3_4(x3_3))\n",
    "        x3_4 = self.l_relu(self.pool(x3_4))\n",
    "\n",
    "        x3_5 = self.l_relu(self.conv3_5(x3_4))\n",
    "\n",
    "        # decode separetely each channel\n",
    "        # First channel\n",
    "        y1_1 = torch.cat((x1_5, x1_4), dim = 1)\n",
    "        y1_1 = self.l_relu(self.upsample(y1_1))\n",
    "        y1_1 = self.l_relu(self.deconv1_1(y1_1))\n",
    "        #print(y1.shape)\n",
    "        y1_2 = torch.cat((y1_1, x1_3), dim = 1)\n",
    "        y1_2 = self.l_relu(self.upsample(y1_2))\n",
    "        y1_2 = self.l_relu(self.deconv1_2(y1_2))\n",
    "        #print(y2.shape)\n",
    "        y1_3 = torch.cat((y1_2, x1_2), dim = 1)\n",
    "        y1_3 = self.l_relu(self.upsample(y1_3))\n",
    "        y1_3 = self.l_relu(self.deconv1_3(y1_3))\n",
    "        #print(y3.shape)\n",
    "        y1_4 = torch.cat((y1_3, x1_1), dim = 1)\n",
    "        y1_4 = self.l_relu(self.upsample(y1_4))\n",
    "        y1_4 = self.l_relu(self.deconv1_4(y1_4))\n",
    "        #print(y4.shape)\n",
    "        y1_5 = torch.cat((y1_4, x1), dim = 1)\n",
    "        #print(y5.shape)\n",
    "        y1_5 = self.l_relu(self.deconv1_5(y1_5))\n",
    "        torch.clamp(y1_5, min = 0, max = 255)\n",
    "\n",
    "        # Second channel\n",
    "        y2_1 = torch.cat((x2_5, x2_4), dim = 1)\n",
    "        y2_1 = self.l_relu(self.upsample(y2_1))\n",
    "        y2_1 = self.l_relu(self.deconv2_1(y2_1))\n",
    "        #print(y1.shape)\n",
    "        y2_2 = torch.cat((y2_1, x2_3), dim = 1)\n",
    "        y2_2 = self.l_relu(self.upsample(y2_2))\n",
    "        y2_2 = self.l_relu(self.deconv2_2(y2_2))\n",
    "        #print(y2.shape)\n",
    "        y2_3 = torch.cat((y2_2, x2_2), dim = 1)\n",
    "        y2_3 = self.l_relu(self.upsample(y2_3))\n",
    "        y2_3 = self.l_relu(self.deconv2_3(y2_3))\n",
    "        #print(y3.shape)\n",
    "        y2_4 = torch.cat((y2_3, x2_1), dim = 1)\n",
    "        y2_4 = self.l_relu(self.upsample(y2_4))\n",
    "        y2_4 = self.l_relu(self.deconv2_4(y2_4))\n",
    "        #print(y4.shape)\n",
    "        y2_5 = torch.cat((y2_4, x2), dim = 1)\n",
    "        #print(y5.shape)\n",
    "        y2_5 = self.l_relu(self.deconv2_5(y2_5))\n",
    "        torch.clamp(y2_5, min = 0, max = 255)\n",
    "\n",
    "        # Thrid channel\n",
    "        y3_1 = torch.cat((x3_5, x3_4), dim = 1)\n",
    "        y3_1 = self.l_relu(self.upsample(y3_1))\n",
    "        y3_1 = self.l_relu(self.deconv3_1(y3_1))\n",
    "        #print(y1.shape)\n",
    "        y3_2 = torch.cat((y3_1, x3_3), dim = 1)\n",
    "        y3_2 = self.l_relu(self.upsample(y3_2))\n",
    "        y3_2 = self.l_relu(self.deconv3_2(y3_2))\n",
    "        #print(y2.shape)\n",
    "        y3_3 = torch.cat((y3_2, x3_2), dim = 1)\n",
    "        y3_3 = self.l_relu(self.upsample(y3_3))\n",
    "        y3_3 = self.l_relu(self.deconv3_3(y3_3))\n",
    "        #print(y3.shape)\n",
    "        y3_4 = torch.cat((y3_3, x3_1), dim = 1)\n",
    "        y3_4 = self.l_relu(self.upsample(y3_4))\n",
    "        y3_4 = self.l_relu(self.deconv3_4(y3_4))\n",
    "        #print(y4.shape)\n",
    "        y3_5 = torch.cat((y3_4, x3), dim = 1)\n",
    "        #print(y5.shape)\n",
    "        y3_5 = self.l_relu(self.deconv3_5(y3_5))\n",
    "        torch.clamp(y3_5, min = 0, max = 255)\n",
    "\n",
    "        y = torch.cat((y1_5, y2_5, y3_5), dim = 1)\n",
    "\n",
    "        return y\n",
    "\n",
    "def plot_3imgs(denoised, ground_truth, noisy_imgs, add_title = ''): #values of the images are in between [0, 255].\n",
    "    plt.subplot(1, 3, 1)\n",
    "    print(noisy_imgs.shape)\n",
    "    plt.imshow(torch.squeeze(noisy_imgs).permute(1, 2, 0).int()) #int since the data has been changed to float for the NN.\n",
    "    plt.title(\"Noisy imgs\")\n",
    "    plt.subplot(1, 3, 2)\n",
    "    plt.imshow(torch.squeeze(ground_truth).permute(1, 2, 0).int())\n",
    "    plt.title(\"Groundtruth\")\n",
    "    plt.subplot(1,3,3)\n",
    "    plt.imshow(torch.squeeze(denoised).permute(1, 2, 0).int())\n",
    "    plt.title(\"Denoised\")\n",
    "    plt.savefig('./ae-small11/' + Losstype + add_title + '.png', dpi = 300)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "import torch\n",
    "from datetime import datetime\n",
    "import time\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "noisy_imgs_1, noisy_imgs_2 = torch.load(\"train_data.pkl\")\n",
    "print(\"Training data : \\n noisiy_imgs_1 : \", noisy_imgs_1.shape, \"\\n noisy_imgs_2 : \", noisy_imgs_2.shape)\n",
    "noisy_imgs, clean_imgs = torch.load(\"val_data.pkl\")\n",
    "print(\"Test data : \\n noisiy_imgs : \", noisy_imgs.shape, \"\\n clean_imgs : \", clean_imgs.shape)\n",
    "noisy_imgs_1_reduced, noisy_imgs_2_reduced = noisy_imgs_1[:SIZE], noisy_imgs_2[:SIZE]\n",
    "print(\"Training data reduced : \\n noisiy_imgs_1_reduced : \", noisy_imgs_1_reduced.shape, \"\\n noisy_imgs_2_reduced : \", noisy_imgs_2_reduced.shape)\n",
    "all_noisy_imgs = torch.cat((noisy_imgs_1_reduced, noisy_imgs_2_reduced), dim = 0)\n",
    "print(\"Concatenated training data (reduced) : \\n all_noisy_imgs : \", all_noisy_imgs.shape)\n",
    "\"\"\"\n",
    "\n",
    "SIZE = 50000\n",
    "BATCH_SIZE = 32\n",
    "train_set = Dataset(SIZE)\n",
    "\n",
    "\"\"\"N = 20\n",
    "plt.figure()\n",
    "for i in range(N):\n",
    "    plt.subplot(2, N, 2*i+1)\n",
    "    plt.imshow(train_set.x[i].permute(1, 2, 0).int())\n",
    "    plt.subplot(2, N, 2*i+2)\n",
    "    plt.imshow(train_set.y[i].permute(1, 2, 0).int())\n",
    "plt.show()\"\"\"\n",
    "\n",
    "\n",
    "# Model Initialization\n",
    "model = AE().to(device)\n",
    "  \n",
    "# Validation using MSE Loss function\n",
    "loss_function = nn.MSELoss().to(device)\n",
    "Losstype = \"L1\"\n",
    "Losstype = \"MSE\"\n",
    "  \n",
    "# Using an Adam Optimizer with lr = 0.001\n",
    "optimizer = torch.optim.Adam(model.parameters(),\n",
    "                             lr = 1e-3, betas=(0.9, 0.99))\n",
    "\n",
    "# DataLoader is used to load the dataset \n",
    "# for training\n",
    "loader_1 = torch.utils.data.DataLoader(dataset = train_set,\n",
    "                                     batch_size = BATCH_SIZE,\n",
    "                                     shuffle = True)\n",
    "\n",
    "\n",
    "#OPTIMIZATION\n",
    "epochs = 10\n",
    "outputs = []\n",
    "losses = []\n",
    "start_ = time.time()\n",
    "for epoch in range(epochs):\n",
    "    print(\"epoch : \", epoch + 1)\n",
    "    start = time.time()\n",
    "    Loss = 0\n",
    "    for noisy_imgs_1, noisy_imgs_2 in loader_1:\n",
    "        #print(noisy_imgs_1.shape)\n",
    "        #print(noisy_imgs_2.shape)\n",
    "\n",
    "        #noisy_imgs_1 = noisy_imgs_1.reshape(-1, 32 * 32)\n",
    "        #noisy_imgs_2 = noisy_imgs_2.reshape(-1, 32 * 32)    \n",
    "        # Output of Autoencoder\n",
    "        #print(\"type : \", noisy_imgs_1.dtype)\n",
    "        noisy_imgs_1, noisy_imgs_2 = noisy_imgs_1.to(device), noisy_imgs_2.to(device)\n",
    "        reconstructed = model(noisy_imgs_1)\n",
    "            \n",
    "        # Calculating the loss function\n",
    "        loss = loss_function(reconstructed, noisy_imgs_2)\n",
    "            \n",
    "        # The gradients are set to zero,\n",
    "        # the the gradient is computed and stored.\n",
    "        # .step() performs parameter update\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # Storing the losses in a list for plotting\n",
    "        Loss += loss.detach().cpu().numpy()\n",
    "    losses.append(Loss)\n",
    "    print('The epoch took {}s to complete\\n'.format(time.time() - start))\n",
    "    outputs.append((epochs, noisy_imgs_2, reconstructed))\n",
    "  \n",
    "# Defining the Plot Style\n",
    "plt.style.use('fivethirtyeight')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "  \n",
    "# Plotting the last 100 values\n",
    "plt.plot(losses)\n",
    "plt.show()\n",
    "\n",
    "print('\\nFreeing some memory...\\n')\n",
    "for noisy1, noisy2 in loader_1:\n",
    "    noisy1 = noisy1.cpu()\n",
    "    noisy2 = noisy2.cpu()\n",
    "\n",
    "\n",
    "\n",
    "Time = datetime.now().strftime('%m_%d_%Hh_%Mm_%Ss')\n",
    "\n",
    "PATH = \"./ae-small11/ae_\" + Losstype + Time + \".pth\" # so that we don't overwrite files\n",
    "torch.save(model.state_dict(), PATH)\n",
    "\n",
    "print('Finished training after {}.'.format(time.time() - start_))\n",
    "print('\\n\\n\\n')\n",
    "print('------------------------------------------------')\n",
    "\n",
    "\n",
    "#model = AE()\n",
    "#time = '04_27_13h_14m_41s' # to be filled according to the job we want to load\n",
    "#PATH = \"./test1/project1_1_\" + time + \".pth\"\n",
    "#model.load_state_dict(torch.load(PATH))\n",
    "\n",
    "\n",
    "print(\"validation\")\n",
    "print('\\n')\n",
    "SIZE = 1000\n",
    "BATCH_SIZE = 1\n",
    "test_set = Dataset(SIZE, train = False)\n",
    "\n",
    "\"\"\"plt.subplot(2, 1, 1)\n",
    "plt.imshow(test_set.x[-1].permute(1, 2, 0).int())\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.imshow(test_set.y[-1].permute(1, 2, 0).int())\n",
    "plt.show()\"\"\"\n",
    "\n",
    "loader_2 = torch.utils.data.DataLoader(dataset = test_set,\n",
    "                                     batch_size = BATCH_SIZE,\n",
    "                                     shuffle = False)\n",
    "\n",
    "PSNR = torch.empty(size = (1, SIZE))\n",
    "i = 0\n",
    "for noisy_imgs, ground_truth in loader_2:\n",
    "    denoised = model(noisy_imgs)\n",
    "    Psnr = psnr(denoised.cpu() / 255, ground_truth.cpu() / 255)\n",
    "    PSNR[0, i] = Psnr\n",
    "    #if Psnr > 32:\n",
    "     #   plot_3imgs(denoised, ground_truth, noisy_imgs, add_title = 'good' + str(i))\n",
    "    #if Psnr < 20:\n",
    "     #   plot_3imgs(denoised, ground_truth, noisy_imgs, add_title = 'bad' + str(i))\n",
    "    i += 1\n",
    "\n",
    "plot_3imgs(denoised.cpu(), ground_truth.cpu(), noisy_imgs.cpu(), add_title = Time)\n",
    "\n",
    "print(\"PSNR mean : \", torch.mean(PSNR).item(), \" dB\")\n",
    "plt.style.use('fivethirtyeight')\n",
    "plt.ylabel('PSNR')\n",
    "plt.plot(PSNR[0,:].detach().numpy())\n",
    "plt.savefig('./ae-small11/psnr_' + Losstype + Time + '.png', dpi = 300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "52af2100627e19856ff19ffeecc72f0dde92b1b054ee87d4964f694fb586b018"
  },
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
