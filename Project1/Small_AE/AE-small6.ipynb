{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"AE-small6.ipynb","provenance":[],"mount_file_id":"1Tp_X1RrcFBVp7ZLmy5lRpubSzLgQ0MgD","authorship_tag":"ABX9TyNTzZaGWPNwyPxJ9SaTqFo8"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":588},"id":"xb9oxYLuVyKG","executionInfo":{"status":"error","timestamp":1651944025520,"user_tz":-120,"elapsed":229773,"user":{"displayName":"Louis Poulain","userId":"00408241798532179809"}},"outputId":"b0722fd1-f7dc-435b-aba2-c68b3a144e51"},"outputs":[{"output_type":"stream","name":"stdout","text":["cpu\n","Training data : \n"," noisy_imgs_1 :  torch.Size([50000, 3, 32, 32]) \n"," noisy_imgs_2 :  torch.Size([50000, 3, 32, 32])\n","Data reduced : \n"," noisy_imgs_1_reduced :  torch.Size([50000, 3, 32, 32]) \n"," noisy_imgs_2_reduced :  torch.Size([50000, 3, 32, 32])\n","Type :  torch.uint8\n","epoch :  1\n","The epoch took 205.1662826538086s to complete\n","\n","epoch :  2\n"]},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-1-2da39260aba3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    206\u001b[0m         \u001b[0mnoisy_imgs_1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnoisy_imgs_1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m         \u001b[0mnoisy_imgs_2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnoisy_imgs_2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 208\u001b[0;31m         \u001b[0mreconstructed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnoisy_imgs_1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    209\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m         \u001b[0;31m# Calculating the loss function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1111\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-1-2da39260aba3>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    111\u001b[0m         \u001b[0my4\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m         \u001b[0my4\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0ml_relu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupsample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 113\u001b[0;31m         \u001b[0my4\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0ml_relu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdeconv4\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    114\u001b[0m         \u001b[0;31m#print(y4.shape)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1111\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    445\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    446\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 447\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    448\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    449\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mConv3d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_ConvNd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    442\u001b[0m                             _pair(0), self.dilation, self.groups)\n\u001b[1;32m    443\u001b[0m         return F.conv2d(input, weight, bias, self.stride,\n\u001b[0;32m--> 444\u001b[0;31m                         self.padding, self.dilation, self.groups)\n\u001b[0m\u001b[1;32m    445\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    446\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}],"source":["# -*- coding: utf-8 -*-\n","\"\"\"\n","Created on Wed Apr 27 15:43:57 2022\n","\n","@author: louis\n","\"\"\"\n","\"file for colab\"\n","import torch \n","# import torch.nn.functional as F\n","import torch.nn as nn\n","import matplotlib.pyplot as plt\n","\n","def psnr(denoised, ground_truth):\n","    # Peak Signal to Noise Ratio: denoised and ground Ì‡truth have range [0, 1]\n","    mse = torch.mean((denoised - ground_truth) ** 2)\n","    return -10 * torch.log10(mse + 10**-8)\n","\n","class Dataset(torch.utils.data.Dataset):\n","  'Characterizes a dataset for PyTorch'\n","  def __init__(self, SIZE, train = True):\n","        'Initialization'\n","        if train: \n","            x, y = torch.load(\"drive/MyDrive/Colab_Notebooks/DeepL_miniProj/train_data.pkl\")\n","            print(\"Training data : \\n noisy_imgs_1 : \", x.shape, \"\\n noisy_imgs_2 : \", y.shape)\n","            if SIZE > 50000:\n","                print(\"You entered a size too big, using size = 50000\")\n","                SIZE = 50000\n","        else : \n","            x, y = torch.load(\"drive/MyDrive/Colab_Notebooks/DeepL_miniProj/val_data.pkl\")\n","            print(\"Test data : \\n noisy_imgs : \", x.shape, \"\\n clean_imgs : \", y.shape)\n","            if SIZE > 50000:\n","                print(\"You entered a size too big, using size = 1000\")\n","                SIZE = 1000\n","        x, y = x[:SIZE], y[:SIZE]\n","        print(\"Data reduced : \\n noisy_imgs_1_reduced : \", x.shape, \"\\n noisy_imgs_2_reduced : \", y.shape)\n","        print(\"Type : \", x.dtype)\n","        self.x = x.float()\n","        self.y = y.float()\n","\n","  def __len__(self):\n","        'Denotes the total number of samples'\n","        return len(self.x)\n","\n","  def __getitem__(self, index):\n","        'Generates one sample of data'\n","        # get label\n","        X = self.x[index]\n","        Y = self.y[index]\n","        return X, Y\n","    \n","class AE(torch.nn.Module):\n","    def __init__(self):\n","        super().__init__()\n","        self.pool = nn.MaxPool2d(kernel_size = 2)\n","        self.conv1 = nn.Conv2d(in_channels = 3, out_channels = 16, kernel_size = 3, padding = 1) # 32 x 32 \n","        self.conv2 = nn.Conv2d(in_channels = 16, out_channels = 24, kernel_size = 3, padding = 1) \n","        self.conv3 = nn.Conv2d(in_channels = 24, out_channels = 48, kernel_size = 3, padding = 1) \n","        self.conv4 = nn.Conv2d(in_channels = 48, out_channels = 48, kernel_size = 3, padding = 1) \n","        self.conv5 = nn.Conv2d(in_channels = 48, out_channels = 48, kernel_size = 3, padding = 1) \n","        \n","        self.deconv1 = nn.Conv2d(in_channels = 144, out_channels = 48, kernel_size = 3, padding = 1) \n","        self.deconv2 = nn.Conv2d(in_channels = 144, out_channels = 48, kernel_size = 3, padding = 1) \n","        self.deconv3 = nn.Conv2d(in_channels = 96, out_channels = 24, kernel_size = 3, padding = 1) \n","        self.deconv4 = nn.Conv2d(in_channels = 56, out_channels = 16, kernel_size = 3, padding = 1) \n","        self.deconv5 = nn.Conv2d(in_channels = 22, out_channels = 3, kernel_size = 3, padding = 1) \n","        \n","        self.l_relu = nn.LeakyReLU(negative_slope = 0.1)\n","        self.upsample = nn.Upsample(scale_factor = (2, 2))\n","        self.linear = nn.Linear(32, 32)\n","        #self.dropout = nn.Dropout(0.5)\n","        \n","        \n","  \n","    def forward(self, x):\n","        # encode\n","        x1 = self.l_relu(self.conv1(x))\n","        x1 = self.l_relu(self.pool(x1))\n","\n","        x2 = self.l_relu(self.conv2(x1))\n","        x2 = self.l_relu(self.pool(x2))\n","\n","        x3 = self.l_relu(self.conv3(x2))\n","        x3 = self.l_relu(self.pool(x3))\n","\n","        x4 = self.l_relu(self.conv4(x3))\n","        x4 = self.l_relu(self.pool(x4))\n","\n","        x5 = self.l_relu(self.conv5(x4))\n","        #print(x5.shape)\n","\n","        # decode\n","        y1 = torch.cat((x5, x4), dim = 1)\n","        y1 = torch.cat((x4, y1), dim = 1)\n","        y1 = self.l_relu(self.upsample(y1))\n","        y1 = self.l_relu(self.deconv1(y1))\n","        #print(y1.shape)\n","\n","        y2 = torch.cat((y1, x3), dim = 1)\n","        y2 = torch.cat((x3, y2), dim = 1)\n","        y2 = self.l_relu(self.upsample(y2))\n","        y2 = self.l_relu(self.deconv2(y2))\n","        #print(y2.shape)\n","\n","        y3 = torch.cat((y2, x2), dim = 1)\n","        y3 = torch.cat((x2, y3), dim = 1)\n","        y3 = self.l_relu(self.upsample(y3))\n","        y3 = self.l_relu(self.deconv3(y3))\n","        #print(y3.shape)\n","\n","        y4 = torch.cat((y3, x1), dim = 1)\n","        y4 = torch.cat((x1, y4), dim = 1)\n","        y4 = self.l_relu(self.upsample(y4))\n","        y4 = self.l_relu(self.deconv4(y4))\n","        #print(y4.shape)\n","\n","        y5 = torch.cat((y4, x), dim = 1)\n","        y5 = torch.cat((x, y5), dim = 1)\n","        #print(y5.shape)\n","        y5 = self.linear(self.deconv5(y5))\n","        \n","        return y5\n","\n","def plot_3imgs(denoised, ground_truth, noisy_imgs, add_title = ''): #values of the images are in between [0, 255].\n","    plt.subplot(1, 3, 1)\n","    print(noisy_imgs.shape)\n","    plt.imshow(torch.squeeze(noisy_imgs).permute(1, 2, 0).int()) #int since the data has been changed to float for the NN.\n","    plt.title(\"Noisy imgs\")\n","    plt.subplot(1, 3, 2)\n","    plt.imshow(torch.squeeze(ground_truth).permute(1, 2, 0).int())\n","    plt.title(\"Groundtruth\")\n","    plt.subplot(1,3,3)\n","    plt.imshow(torch.squeeze(denoised).permute(1, 2, 0).int())\n","    plt.title(\"Denoised\")\n","    plt.savefig('drive/MyDrive/Colab_Notebooks/DeepL_miniProj/ae-small6/' + Losstype + add_title + '.png', dpi = 300)\n","    plt.show()\n","\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","print(device)\n","import torch\n","from datetime import datetime\n","import time\n","\n","\n","\"\"\"\n","noisy_imgs_1, noisy_imgs_2 = torch.load(\"train_data.pkl\")\n","print(\"Training data : \\n noisiy_imgs_1 : \", noisy_imgs_1.shape, \"\\n noisy_imgs_2 : \", noisy_imgs_2.shape)\n","noisy_imgs, clean_imgs = torch.load(\"val_data.pkl\")\n","print(\"Test data : \\n noisiy_imgs : \", noisy_imgs.shape, \"\\n clean_imgs : \", clean_imgs.shape)\n","noisy_imgs_1_reduced, noisy_imgs_2_reduced = noisy_imgs_1[:SIZE], noisy_imgs_2[:SIZE]\n","print(\"Training data reduced : \\n noisiy_imgs_1_reduced : \", noisy_imgs_1_reduced.shape, \"\\n noisy_imgs_2_reduced : \", noisy_imgs_2_reduced.shape)\n","all_noisy_imgs = torch.cat((noisy_imgs_1_reduced, noisy_imgs_2_reduced), dim = 0)\n","print(\"Concatenated training data (reduced) : \\n all_noisy_imgs : \", all_noisy_imgs.shape)\n","\"\"\"\n","\n","SIZE = 50000\n","BATCH_SIZE = 128\n","train_set = Dataset(SIZE)\n","\n","\"\"\"N = 20\n","plt.figure()\n","for i in range(N):\n","    plt.subplot(2, N, 2*i+1)\n","    plt.imshow(train_set.x[i].permute(1, 2, 0).int())\n","    plt.subplot(2, N, 2*i+2)\n","    plt.imshow(train_set.y[i].permute(1, 2, 0).int())\n","plt.show()\"\"\"\n","\n","\n","# Model Initialization\n","model = AE().to(device)\n","  \n","# Validation using MSE Loss function\n","loss_function = nn.L1Loss().to(device)\n","Losstype = \"L1\"\n","#Losstype = \"MSE\"\n","  \n","# Using an Adam Optimizer with lr = 0.001\n","optimizer = torch.optim.Adam(model.parameters(),\n","                             lr = 1e-3, betas=(0.9, 0.99))\n","\n","# DataLoader is used to load the dataset \n","# for training\n","loader_1 = torch.utils.data.DataLoader(dataset = train_set,\n","                                     batch_size = BATCH_SIZE,\n","                                     shuffle = True)\n","\n","\n","#OPTIMIZATION\n","epochs = 10\n","outputs = []\n","losses = []\n","start_ = time.time()\n","for epoch in range(epochs):\n","    print(\"epoch : \", epoch + 1)\n","    start = time.time()\n","    Loss = 0\n","    for noisy_imgs_1, noisy_imgs_2 in loader_1:\n","        #print(noisy_imgs_1.shape)\n","        #print(noisy_imgs_2.shape)\n","\n","        #noisy_imgs_1 = noisy_imgs_1.reshape(-1, 32 * 32)\n","        #noisy_imgs_2 = noisy_imgs_2.reshape(-1, 32 * 32)    \n","        # Output of Autoencoder\n","        #print(\"type : \", noisy_imgs_1.dtype)\n","        noisy_imgs_1 = noisy_imgs_1.to(device)\n","        noisy_imgs_2 = noisy_imgs_2.to(device)\n","        reconstructed = model(noisy_imgs_1)\n","            \n","        # Calculating the loss function\n","        loss = loss_function(reconstructed, noisy_imgs_2)\n","            \n","        # The gradients are set to zero,\n","        # the the gradient is computed and stored.\n","        # .step() performs parameter update\n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","        # Storing the losses in a list for plotting\n","        Loss += loss.detach().cpu().numpy()\n","    losses.append(Loss)\n","    print('The epoch took {}s to complete\\n'.format(time.time() - start))\n","    outputs.append((epochs, noisy_imgs_2, reconstructed))\n","  \n","# Defining the Plot Style\n","plt.style.use('fivethirtyeight')\n","plt.xlabel('Epochs')\n","plt.ylabel('Loss')\n","  \n","# Plotting the last 100 values\n","plt.plot(losses)\n","plt.show()\n","\n","\n","Time = datetime.now().strftime('%m_%d_%Hh_%Mm_%Ss')\n","\n","PATH = \"drive/MyDrive/Colab_Notebooks/DeepL_miniProj/ae-small6/ae_\" + Losstype + Time + \".pth\" # so that we don't overwrite files\n","torch.save(model.state_dict(), PATH)\n","\n","print('Finished training after {}.'.format(time.time() - start_))\n","print('\\n\\n\\n')\n","print('------------------------------------------------')\n","\n","\n","#model = AE()\n","#time = '04_27_13h_14m_41s' # to be filled according to the job we want to load\n","#PATH = \"./test1/project1_1_\" + time + \".pth\"\n","#model.load_state_dict(torch.load(PATH))\n","\n","\n","print(\"validation\")\n","print('\\n')\n","SIZE = 1000\n","BATCH_SIZE = 1\n","test_set = Dataset(SIZE, train = False)\n","\n","\"\"\"plt.subplot(2, 1, 1)\n","plt.imshow(test_set.x[-1].permute(1, 2, 0).int())\n","plt.subplot(2, 1, 2)\n","plt.imshow(test_set.y[-1].permute(1, 2, 0).int())\n","plt.show()\"\"\"\n","\n","loader_2 = torch.utils.data.DataLoader(dataset = test_set,\n","                                     batch_size = BATCH_SIZE,\n","                                     shuffle = False)\n","\n","PSNR = torch.empty(size = (1, SIZE))\n","i = 0\n","for noisy_imgs, ground_truth in loader_2:\n","    noisy_imgs = noisy_imgs.to(device)\n","    ground_truth = ground_truth.to(device)\n","    denoised = model(noisy_imgs)\n","    Psnr = psnr(denoised.cpu() / 255, ground_truth.cpu() / 255)\n","    PSNR[0, i] = Psnr\n","    #if Psnr > 32:\n","     #   plot_3imgs(denoised, ground_truth, noisy_imgs, add_title = 'good' + str(i))\n","    #if Psnr < 20:\n","     #   plot_3imgs(denoised, ground_truth, noisy_imgs, add_title = 'bad' + str(i))\n","    i += 1\n","\n","plot_3imgs(denoised.cpu(), ground_truth.cpu(), noisy_imgs.cpu(), add_title = Time)\n","\n","print(\"PSNR mean : \", torch.mean(PSNR).item(), \" dB\")\n","plt.style.use('fivethirtyeight')\n","plt.ylabel('PSNR')\n","plt.plot(PSNR[0,:].detach().numpy())\n","plt.savefig('drive/MyDrive/Colab_Notebooks/DeepL_miniProj/ae-small6/psnr_' + Losstype + Time + '.png', dpi = 300)\n","plt.show()"]},{"cell_type":"code","source":[""],"metadata":{"id":"Ku6trvkGcBbW"},"execution_count":null,"outputs":[]}]}